---
---
References
==========

@inproceedings{scieur2016regularized,
  title={Regularized Nonlinear Acceleration},
  author={Scieur, Damien and d'Aspremont, Alexandre and Bach, Francis},
  booktitle={Advances In Neural Information Processing Systems},
  pages={712--720},
  year={2016},
  paperpdf={6267-regularized-nonlinear-acceleration.pdf},
  abstract={We defaultscribe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.}
}


@techreport{scieur2017integration,
  title={Integration Methods and Accelerated Optimization Algorithms},
  author={Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre},
  journal={arXiv preprint arXiv:1702.06751},
  year={2017},
  paperpdf={Integration Methods and Acceleratedated Optimization Algorithms.pdf},
  abstract={We show that Accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. In comparison with recent advances in this vein, the differential equation considered here is the basic gradient flow and we show that multi-step schemes allow integration of this differential equation using larger step sizes, thus intuitively explaining acceleration results.}
}


@mastersthesis{scieur2015master,
  title={Global Complexity Analysis For The Second-Order Methods},
  author={Scieur, Damien},
  year={2015},
  month = {jun},
  school = {Universit\'e catholique de Louvain},
  paperpdf={masterthesis_scieur_damien.pdf},
  abstract={The goal of this mastersthesiser thesis will be firstly to analyze with precision the behavior of the cubic regularization of Newton's method on strongly convex functions. During this analysis we will find that the regularisation does not work as well as expected when the function is also smooth, unlike the gradient method. We will thus propose some variants of the original algorithm in order to have better global performances.}
}
